{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gugupig/bert_WSD/blob/main/Transformers_wsd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp3IN9YQexxx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from tqdm import tqdm  \n",
        "from tqdm.notebook import tqdm # for progress bars in notebooks\n",
        "from random import shuffle\n",
        "import os\n",
        "import sys\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_c3C5Pzexx-"
      },
      "source": [
        "## Naming conventions\n",
        "\n",
        "- sentences are already segmented into words (with a rule-based tokenizer)\n",
        "- but are not segmented into subwords yet\n",
        "- we use \"word\" or \"w\" for the tokens obtained after pre-segmentation\n",
        "- and \"token\" for units obtained after *BERT*-like tokenization (BPE ou WordPiece etc...)\n",
        "\n",
        "- in variable names, we distinguish \n",
        " - integer identifiers for symbols \n",
        "   (for the token vocabulary, the frame vocabulary ...)\n",
        " - versus the rank of a unit (either word or token) within a sequence\n",
        "- tid => token identifier\n",
        "- trk / wrk => token rank / rank of a word in a sequence\n",
        "- tg => \"target\", so \n",
        " - tg_wrk = rank of the target word\n",
        " - tg_trk = rank of the first token of the target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1clXe8e2SBta",
        "outputId": "ac39d736-d08a-4498-f30a-b210ef07da51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use GPU 0 (Tesla P100-PCIE-16GB) of compute capability 6.0 with 17.07Gb total memory.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# in order to use a GPU\n",
        "# modify notebook settings:\n",
        "# Edit > Notebook settings > Hardware accelerator => select \"GPU\"\n",
        "\n",
        "# if a GPU is available, we will use it\n",
        "if torch.cuda.is_available():\n",
        "    # objet torch.device          \n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "        \n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
        "    device_id = torch.cuda.current_device()\n",
        "    gpu_properties = torch.cuda.get_device_properties(device_id)\n",
        "    print(\"We will use GPU %d (%s) of compute capability %d.%d with \"\n",
        "          \"%.2fGb total memory.\\n\" % \n",
        "          (device_id,\n",
        "          gpu_properties.name,\n",
        "          gpu_properties.major,\n",
        "          gpu_properties.minor,\n",
        "          gpu_properties.total_memory / 1e9))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfm0CXb9exyF"
      },
      "source": [
        "## \"ASFALDA\" dataset\n",
        "\n",
        "A French FrameNet, comprisong about 16000 annotated targets, into about 100 distinct frames, along with their semantic role annotations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kySX0ye3jdpP"
      },
      "source": [
        "### Fetching the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRkusCSCjg8I"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./asfalda_data_for_wsd/'):\n",
        "  # shell commands can be run using !\n",
        "  !pip install wget\n",
        "  import wget\n",
        "  \n",
        "  # The URL for the dataset zip file.\n",
        "  url = 'http://www.linguist.univ-paris-diderot.fr/~mcandito/divers/asfalda_data_for_wsd.tgz'\n",
        "\n",
        "  \n",
        "  if not os.path.exists('./asfalda_data_for_wsd.tgz'):\n",
        "    print('Downloading dataset')\n",
        "    wget.download(url, './asfalda_data_for_wsd.tgz')\n",
        "    !tar zxf asfalda_data_for_wsd.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_2cjFvYfOn3"
      },
      "source": [
        "### Data loading method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYQhZaBzexyF"
      },
      "outputs": [],
      "source": [
        "def load_asfalda_data(gold_data_file, split_info_file):\n",
        "    \"\"\"\n",
        "        Inputs: - asfalda gold data file\n",
        "                - file indicating the corpus type for each sentence id\n",
        "\n",
        "        Returns 3 dictionaries (whose keys are corpus types (train/dev/test/val))\n",
        "        - sentences\n",
        "        - list of rank of target word in each sentence\n",
        "        - gold labels\n",
        "\n",
        "        Example:\n",
        "        sentences['train'] = [['Le', 'code', 'comprend', 'des', 'erreurs','.'],\n",
        "                              ['Comprends', '-tu', '?']]\n",
        "         # the targets are the 3rd and first words                     \n",
        "        tg_wrks['train'] = [2, 0]\n",
        "        tg_lemmas['train'] = ['comprendre', 'comprendre']\n",
        "        labels = ['frame1', 'frame2']\n",
        "                                \n",
        "    \"\"\"\n",
        "    # load the usual split into train / dev / test\n",
        "    s = open(split_info_file)\n",
        "    lines = [ l[:-1].split('\\t') for l in s.readlines() ]\n",
        "    split_info_dic = { line[0]:line[1] for line in lines }\n",
        "\n",
        "    # dev / train / test sentences\n",
        "    sentences = {'dev':[], 'train':[], 'test':[]}\n",
        "    # the word ranks (wrk) for the target words\n",
        "    tg_wrks = {'dev':[], 'train':[], 'test':[]}\n",
        "    # target lemmas\n",
        "    tg_lemmas = {'dev':[], 'train':[], 'test':[]}\n",
        "    # the labels of targets (= frames)\n",
        "    labels = {'dev':[], 'train':[], 'test':[]}\n",
        "    tg_poss = {'dev':[], 'train':[], 'test':[]}\n",
        "    max_sent_len = {'dev':0, 'train':0, 'test':0}\n",
        "    max_tg_wrk = {'dev':0, 'train':0, 'test':0}\n",
        "\n",
        "    stream = open(gold_data_file)\n",
        "    for line in stream.readlines():\n",
        "        if line.startswith('#'):\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        (sentid, tg_wrk, frame_name, tg_lemma, tg_pos, rest) = line.split('\\t',5)\n",
        "        # role annotation is ignored\n",
        "        # sentences are pre-segmented into space-separated words\n",
        "        # => we split, to use the is_split_into_words=True mode of the FlauBERT tokenizer\n",
        "        sentence = rest.split(\"\\t\")[-1].split(' ')\n",
        "        part = split_info_dic[sentid]\n",
        "        tg_wrk = int(tg_wrk)\n",
        "\n",
        "        l = len(sentence)\n",
        "        sentences[part].append(sentence)\n",
        "        labels[part].append(frame_name)\n",
        "        tg_wrks[part].append(tg_wrk)\n",
        "        tg_lemmas[part].append(tg_lemma)\n",
        "        tg_poss[part].append(tg_pos)\n",
        "        if max_sent_len[part] < l: \n",
        "            max_sent_len[part] = l \n",
        "        if max_tg_wrk[part] < tg_wrk: \n",
        "            max_tg_wrk[part] = tg_wrk \n",
        "    print(\"Max sentence length:\", max_sent_len)\n",
        "    print(\"Max target rank (in words):\", max_tg_wrk)\n",
        "    \n",
        "    return sentences, tg_wrks, tg_lemmas, labels,tg_poss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo-B32y4c308"
      },
      "source": [
        "### Data loading and defining ids for labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaKnFVJ0exyL",
        "outputId": "650058da-ad41-40f1-e1a8-1dab2e77e645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length: {'dev': 115, 'train': 271, 'test': 140}\n",
            "Max target rank (in words): {'dev': 96, 'train': 267, 'test': 115}\n",
            "dev : 2688 sentences, average lentgh=38.03\n",
            "train : 18657 sentences, average lentgh=38.99\n",
            "test : 3447 sentences, average lentgh=38.45\n"
          ]
        }
      ],
      "source": [
        "MAX_LENGTH = 100\n",
        "gold_data_file = './asfalda_data_for_wsd/sequoiaftb.asfalda_1_3.gold.uniq.nofullant.txt'\n",
        "\n",
        "# usual split train / dev / test for this corpus\n",
        "split_info_file = './asfalda_data_for_wsd/sequoiaftb_split_info'\n",
        "\n",
        "sentences, tg_wrks, tg_lemmas, label_strs,tg_pos = load_asfalda_data(gold_data_file,split_info_file)\n",
        "\n",
        "\n",
        "for p in sentences.keys():\n",
        "    avgl = sum([len(s) for s in sentences[p]])/len(sentences[p])\n",
        "    print(\"%s : %d sentences, average lentgh=%3.2f\" \n",
        "          %(p, len(sentences[p]), avgl))\n",
        "\n",
        "# creating label ids for frames seen in training set\n",
        "i2label = list(set(label_strs['train']))\n",
        "# id for unknown frame (for dev and test)\n",
        "i2label.append('*UNK*')\n",
        "\n",
        "label2i = {x:i for i,x in enumerate(i2label)}\n",
        "# id of special frame \"Other_sense\"\n",
        "i_OTHER_SENSE = label2i['Other_sense']\n",
        "\n",
        "# sequence of gold labels \n",
        "# for each sub-corpus (key = dev/train/test)\n",
        "labels = {}\n",
        "for p in label_strs.keys():\n",
        "    labels[p] = [label2i[x] if x in label2i else i2label[-1] for x in label_strs[p]]\n",
        "\n",
        "i2pos = list(set(tg_pos['train']))\n",
        "i2pos.append('*UNK*')\n",
        "pos2i = {x:i for i,x in enumerate(i2pos)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE28jS8Jgqrz"
      },
      "source": [
        "## Data encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPbOlFK9nLoU"
      },
      "source": [
        "### FlauBERT tokenization\n",
        "\n",
        "We use the FlauBERT model, using the Huggingface \"transformers\" module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4DfehySexyR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import transformers\n",
        "except ImportError:\n",
        "  !pip install transformers\n",
        "  \n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJiKhfxhexyV"
      },
      "outputs": [],
      "source": [
        "# We choose the FlauBERT model\n",
        "\n",
        "# we load tokenizer and config for now\n",
        "flaubert_tokenizer = AutoTokenizer.from_pretrained(\"flaubert/flaubert_base_cased\")\n",
        "flaubert_config = AutoConfig.from_pretrained(\"flaubert/flaubert_base_cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ENCODING"
      ],
      "metadata": {
        "id": "OxIjNEutBFpi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnAwvhqQ5RCQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "some target ranks (before or after BPE tokenization) are bigger than the max length of the tid_seq.\n",
        "to solve this problem, some samples with out-of-range rank positions will be discarded, since this operation will mess up the original order, \n",
        "the encode function needs the original lemma and label lists and its outputs also will include new lemma and label lists \n",
        "'''\n",
        "class WSDEncoder:\n",
        "    def __init__(self, tokenizer, config):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config # to get indices of special tokens\n",
        "\n",
        "    def encode(self, sentences, tg_wrks, lemmas = None,labels = None,max_length=100, verbose=False, is_split_into_words=True):\n",
        " \n",
        "      if is_split_into_words:\n",
        "        sentences_ = sentences\n",
        "        sentences_join = [' '.join(sentence) for sentence in sentences]\n",
        "        tid_seqs_ = flaubert_tokenizer(sentences_join,truncation=True,padding=True,max_length=max_length,add_special_tokens=True)['input_ids']\n",
        "      else:\n",
        "        sentences_ = [sentence.split(' ') for sentence in sentences] \n",
        "        tid_seqs_ = flaubert_tokenizer(sentences,truncation=True,padding=True,max_length=max_length,add_special_tokens=True)['input_ids']\n",
        "      first_trk_of_targets = []\n",
        "      tid_seqs = []\n",
        "      tid_lemmas = []\n",
        "      tid_labels = []\n",
        "      discard_counter = 0\n",
        "      discarded = ''\n",
        "      if lemmas and labels:\n",
        "        for sentence,wrk,seq,lem,lab in zip(sentences_,tg_wrks,tid_seqs_,lemmas,labels):\n",
        "          target = sentence[wrk]\n",
        "          if target[-1] == \"'\":\n",
        "            target = target[:-1]\n",
        "          encoded_word = flaubert_tokenizer.encode(target)[1:-1][0]\n",
        "          if encoded_word in seq:\n",
        "            encoded_word_index = seq.index(encoded_word)\n",
        "            first_trk_of_targets.append(encoded_word_index)\n",
        "            tid_seqs.append(seq)\n",
        "            tid_lemmas.append(lem)\n",
        "            tid_labels.append(lab)\n",
        "          else :\n",
        "            discard_counter += 1\n",
        "            discarded = discarded + ' '  + sentence[wrk]\n",
        "            sys.stdout.write('\\rRank position is bigger than max length,this sample will be discarded : ' + discarded)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "# the encode function did not tokenize word per word, instead, it tokenized the whole sentence.\n",
        "# So, in order to get the target rank after tokenization, only the target word is tokenized to get its encoded id \n",
        "# and seq.index function is used to get the correct target rank in the tokenized sentence\n",
        "\n",
        "\n",
        "      elif lemmas == None and labels == None:\n",
        "          for sentence,wrk,seq in zip(sentences_,tg_wrks,tid_seqs_):\n",
        "            target = sentence[wrk]\n",
        "# some target word like \" d' \" is end with \" ' \",which the sentence tokenizer will treat it as two tokens : \" d \"  and \" ' \"  \n",
        "# but the word tokenizer treat it as a single word         \n",
        "            if target[-1] == \"'\": \n",
        "              target = target[:-1]\n",
        "            encoded_word = flaubert_tokenizer.encode(target)[1:-1][0]\n",
        "            if encoded_word in seq:\n",
        "              encoded_word_index = seq.index(encoded_word)\n",
        "              first_trk_of_targets.append(encoded_word_index)\n",
        "              tid_seqs.append(seq)\n",
        "            else :\n",
        "              discard_counter += 1\n",
        "              print('\\rRank position is bigger than max length,this sample will be discarded')\n",
        "              print(sentence[wrk])\n",
        "              print('==============================')          \n",
        "      print(f\"\\n{discard_counter}/{len(sentences_)} samples has been discarded \")\n",
        "      if labels and lemmas :\n",
        "        assert len(tid_seqs) == len(first_trk_of_targets) == len(tid_lemmas) == len(tid_labels)\n",
        "        result = (tid_seqs,first_trk_of_targets,tid_lemmas,tid_labels)\n",
        "      else:\n",
        "        assert len(tid_seqs) == len(first_trk_of_targets) \n",
        "        result = (tid_seqs,first_trk_of_targets)\n",
        "      return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySTnpTLexyi"
      },
      "source": [
        "### Encoding test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fK8CV5Bexyj",
        "outputId": "90127750-0639-4f26-8490-ff4d990ac543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0/4 samples has been discarded \n",
            "Len = 10 target token rank = 6 tid_seq = [0, 1198, 17358, 13299, 14, 65, 18719, 1999, 19614, 1]\n",
            "compr\n",
            "Len = 10 target token rank = 3 tid_seq = [0, 55, 1138, 976, 23, 3842, 16, 1, 2, 2]\n",
            "comprend</w>\n",
            "Len = 10 target token rank = 5 tid_seq = [0, 2684, 68, 5213, 15, 965, 22, 14659, 896, 1]\n",
            "comprendre</w>\n",
            "Len = 10 target token rank = 7 tid_seq = [0, 59, 261, 68, 34, 42, 83, 681, 20, 1]\n",
            "compris</w>\n"
          ]
        }
      ],
      "source": [
        "encoder = WSDEncoder(flaubert_tokenizer, flaubert_config)\n",
        "\n",
        "# test encoder\n",
        "test_sents = [\"Conséquemment , nous comprendrions .\",\n",
        "              \"Le code comprend des erreurs .\",\n",
        "            \"J' essaie de comprendre les transformers .\",  \n",
        "            \"Il n' a pas bien compris le code !\"]\n",
        "# target words are the occurrences of \"comprendre\"\n",
        "test_tg_wrks = [3, 2, 3, 5]\n",
        "max_length=10\n",
        "\n",
        "tid_seqs, first_trk_of_targets = encoder.encode(test_sents, test_tg_wrks, max_length= 10, verbose=True,is_split_into_words=False)\n",
        "\n",
        "for tid_seq, ft in zip(tid_seqs, first_trk_of_targets):\n",
        "    print(\"Len = %d target token rank = %d tid_seq = %s\" % (len(tid_seq), ft, str(tid_seq)))\n",
        "    print(flaubert_tokenizer.convert_ids_to_tokens(tid_seq)[ft])\n",
        "#flaubert_tokenizer.convert_ids_to_tokens(tid_seqs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHApJ8mgexyn"
      },
      "source": [
        "### Full encoding and batch production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMxpzaBhexyo"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class WSDData:\n",
        "    def __init__(self, corpus_type, sentences, tg_wrks, tg_lemmas, labels, encoder, max_length=100):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - corpus type string (train/dev/test/val)\n",
        "        - list of sentences (each sentence = list of word strings)\n",
        "        - list of target word ranks : one per sentence\n",
        "        - list of gold label id\n",
        "        - encoder = instance of WSDEncoder\n",
        "\n",
        "        - max_length = size of encoded sequences, in nb of bert tokens \n",
        "                      (padded / truncated via encoder.encode)\n",
        "    \n",
        "        Encodes all the data using the relevant identifiers\n",
        "        \"\"\"\n",
        "        \n",
        "        self.corpus_type = corpus_type # train / dev / test / val\n",
        "        self.size = len(sentences)\n",
        "        self.encoder = encoder\n",
        "\n",
        "        \n",
        "        self.sentences = sentences # list of list of word strings\n",
        "        \n",
        "        \n",
        "        tid_seqs, tg_trks,lemmas,labels = encoder.encode(sentences, tg_wrks,tg_lemmas,labels, max_length = max_length ,is_split_into_words=True)\n",
        "\n",
        "        self.tid_seqs = tid_seqs  # sequences of token ids\n",
        "        self.tg_trks = tg_trks    # target token ranks\n",
        "        self.tg_lemmas = lemmas\n",
        "        self.labels = labels       # gold label ids\n",
        "\n",
        "    def shuffle(self):\n",
        "      seq_tg_pairs = list(zip(self.tid_seqs,self.tg_trks,self.tg_lemmas,self.labels))\n",
        "      random.shuffle(seq_tg_pairs)\n",
        "      ts,tt,tle,tla = zip(*seq_tg_pairs)\n",
        "      self.tid_seqs,self.tg_trks,self.tg_lemmas,self.labels= list(ts),list(tt),list(tle),list(tla)\n",
        "      assert len(self.tid_seqs) == len(self.tg_trks) == len(self.tg_lemmas) == len(self.labels)\n",
        "      \"\"\"\n",
        "      Rearranges all the data in a new random order\n",
        "      (sentences, tg_lemmas, tg_trks, tid_seqs, labels)\n",
        "\n",
        "      NB: ** original order is lost **\n",
        "      \"\"\"\n",
        "\n",
        "    # production of a batch\n",
        "    def make_batches(self, batch_size, shuffle_data=False):\n",
        "      assert len(self.tid_seqs) == len(self.tg_trks) == len(self.tg_lemmas) == len(self.labels)\n",
        "      if shuffle_data:\n",
        "        self.shuffle()\n",
        "      if batch_size > len(self.tid_seqs):\n",
        "        raise ValueError('Batch size is bigger than data size!!')\n",
        "      for x in range(0,len(self.tid_seqs),batch_size):\n",
        "        yield (self.tid_seqs[x:x+batch_size],self.tg_trks[x:x+batch_size],self.tg_lemmas[x:x+batch_size],self.labels[x:x+batch_size])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUj24OEF6_Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c319f77f-3d49-483f-b4c7-4b2b0625b2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding part dev ...\n",
            "Rank position is bigger than max length,this sample will be discarded :  produit considérés réalisées\n",
            "3/2688 samples has been discarded \n",
            "Encoding part train ...\n",
            "Rank position is bigger than max length,this sample will be discarded :  produit résultant concevez dites opposition faire semble impulsion sait applaudir conviction coûtent ouverte reconnue vu vu contraints accusent créations escomptées Paye laissant répondent conditions laissent contraintes dites entamée reprenant déclinait interprétation assureront répondront écrit réclamations régler demandes perdre raisons fonctions invité sentiment vu ordre accréditer idée conséquences refusa toucher soupçonnant fonctions vu anathème prononcer discours\n",
            "55/18657 samples has been discarded \n",
            "Encoding part test ...\n",
            "Rank position is bigger than max length,this sample will be discarded :  assurer cité céder ventes fondent favorables décidées attirer\n",
            "8/3447 samples has been discarded \n"
          ]
        }
      ],
      "source": [
        "MAX_LENGTH = 100\n",
        "wsd_data = {}\n",
        "# key = part of the split corpus (train/test/dev)\n",
        "for p in sentences.keys():\n",
        "    print(\"Encoding part %s ...\" % p)\n",
        "    wsd_data[p] = WSDData(p, sentences[p], tg_wrks[p], tg_lemmas[p], labels[p], \n",
        "                          encoder, max_length=MAX_LENGTH)\n",
        "    # we check that encoding provides the right lengths\n",
        "    for i, s in enumerate(wsd_data[p].tid_seqs):\n",
        "        if len(s) != MAX_LENGTH:\n",
        "            print(\"Size bug:\", i, s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIw1BE1wexys"
      },
      "source": [
        "## WSDClassifier class: the network for WSD\n",
        "\n",
        "Base architecture = \n",
        "- the FlauBERT model\n",
        "- plus linear layer + softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ-w08okgaXv"
      },
      "source": [
        "The network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Vpgmjmkn88",
        "outputId": "d517efc6-cb60-42dd-d2b9-4ff06425a490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
          ]
        }
      ],
      "source": [
        "flaubert_model = AutoModel.from_pretrained(\"flaubert/flaubert_base_cased\", return_dict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFQEpfkRexyy"
      },
      "outputs": [],
      "source": [
        "class WSDClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_labels, device, bert_model, bert_config,freeze_bert = True,MLP = False):\n",
        "        super(WSDClassifier, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "                \n",
        "        # the full *BERT*-like model\n",
        "        # the .to(device) triggers the copy towards the relevant device \n",
        "        # (possibly a GPU)\n",
        "        self.bert_layer = bert_model.to(device)\n",
        "        # config will allow to get the hidden vectors' size\n",
        "        self.bert_config = bert_config\n",
        "        self.num_labels = num_labels\n",
        "        self.emb_size = self.bert_config.emb_dim\n",
        "        \n",
        "        self.distribution = nn.Sequential (*[nn.Linear(in_features = self.emb_size,out_features=self.num_labels),\n",
        "                                            nn.LogSoftmax(dim = -1)\n",
        "                                           \n",
        "        ]).to(device)\n",
        "        if MLP:\n",
        "          self.distribution = nn.Sequential (*[nn.Linear(in_features = self.emb_size,out_features=50),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(in_features = 50,out_features=50),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(in_features = 50,out_features=self.num_labels),\n",
        "                                            nn.LogSoftmax(dim = -1)]).to(device)\n",
        "        \n",
        "        if freeze_bert:\n",
        "          for param in self.bert_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # make a mask matrix (batch_size,nb_class)\n",
        "    # where only the seen in train frames are 0s and the not seen in train frame are 1e-45 (or -inf)\n",
        "    def make_mask(self,nb_class,lemmas,seen_in_X):\n",
        "      batch_size = len(lemmas)\n",
        "      zeros = torch.zeros(batch_size,nb_class,device = self.device,requires_grad=False)\n",
        "      zeros = zeros + 1e-45 #(-1)*float('inf')\n",
        "      for i in range(batch_size):\n",
        "        seen = seen_in_X[lemmas[i]]\n",
        "        for j in seen:\n",
        "          zeros[i][j] = 0\n",
        "      return zeros\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, b_tid_seq, b_tg_trk,lemmas = None,seen_in_X = None):\n",
        "      bert_out = self.bert_layer(b_tid_seq,return_dict =True).last_hidden_state\n",
        "      #b_tg_trk = torch.tensor(b_tg_trk, device=self.device)\n",
        "      target = bert_out[torch.arange(bert_out.size(0)),b_tg_trk]\n",
        "      out = self.distribution(target)\n",
        "      if lemmas and seen_in_X:\n",
        "    # if lemmas and seen_in_X is provided,mask the output\n",
        "    # so only the seen in train frames position has predicted valued\n",
        "    # other position are -inf or some very extreme negative values\n",
        "        mask = self.make_mask(self.num_labels,lemmas,seen_in_X)\n",
        "        #l1_norm = (-1*(torch.norm(out+mask,p=1, dim=-1))).view(1,-1)\n",
        "        exp = torch.exp(out+mask) # turn it back to the normal probability to calculate the l1 norm\n",
        "        l1_norm = torch.sum(exp,dim = -1).view(1,-1) \n",
        "        out = ((exp)/(l1_norm.unsqueeze(2))).squeeze() # use the l1 norm to replace an another log softmax for re-normalization\n",
        "        out = torch.log(out)\n",
        "        #out = self.log_softmax(out+mask)\n",
        "\n",
        "      return out\n",
        "    \"\"\"\n",
        "    Inputs: (all are tensors, on the relevant device)\n",
        "        - a batch of sentences = a batch of token id sequences \n",
        "          (as output in 'input_ids' member of tokenizer output)\n",
        "        - a batch of target token rank = for each of the sentences, \n",
        "          the rank of first token of the target word to disambiguate\n",
        "\n",
        "    Output: log_softmax scores for the whole batch (batch_size x num_labels)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "            \n",
        "    def run_on_dataset(self, wsd_data, return_loss = False,batch_size=32):\n",
        "      pred_labels = []\n",
        "      gold = []\n",
        "      loss  = []\n",
        "      self.eval()\n",
        "      with torch.no_grad():\n",
        "        for b_tid_seqs, b_tg_trks,_,b_labels in wsd_data.make_batches(batch_size, shuffle_data=False):\n",
        "          b_tid_seqs = torch.tensor(b_tid_seqs, device=self.device)\n",
        "          b_tg_trks = torch.tensor(b_tg_trks, device=self.device)\n",
        "          b_labels = torch.tensor(b_labels, device=self.device)\n",
        "          log_probs = self.forward(b_tid_seqs, b_tg_trks)\n",
        "          pred_label = torch.argmax(log_probs,dim = -1)\n",
        "          pred_labels.append(pred_label)\n",
        "          gold.append(b_labels)\n",
        "          if return_loss:\n",
        "            loss.append(torch.nn.functional.nll_loss(log_probs,b_labels).item())\n",
        "            \n",
        "      if return_loss:\n",
        "        return pred_labels,gold,loss\n",
        "      else:\n",
        "        return pred_labels,gold\n",
        "\n",
        "      \"\"\"\n",
        "        Run classifier on wsd_data and compute accuracy\n",
        "        Inputs = \n",
        "         - wsd_data (WSDDataset instance)\n",
        "         - batch_size\n",
        "        Returns:\n",
        "         - list of predicted label ids\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "    def evaluate(self,wsd_data,return_loss = False,batch_size=32):\n",
        "        \"\"\" returns accuracy, nb_correct, nb_total \"\"\"\n",
        "        nb_correct = 0\n",
        "        nb_total = 0\n",
        "        result = self.run_on_dataset(wsd_data,return_loss ,batch_size)\n",
        "        for p,g in zip(result[0],result[1]):\n",
        "          nb_correct += torch.sum(p==g)\n",
        "          nb_total += len(p)\n",
        "        if return_loss:\n",
        "          return float(nb_correct/nb_total),float(len(result[2])/sum(result[2]))\n",
        "        else:\n",
        "          return float(nb_correct/nb_total)\n",
        "        \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrkbQ4hVexy1"
      },
      "outputs": [],
      "source": [
        "# an instance of WSDClassifier\n",
        "num_labels = len(i2label)\n",
        "DEVICE = 'cuda'\n",
        "classifier = WSDClassifier(num_labels, DEVICE, flaubert_model, flaubert_config)\n",
        "\n",
        "# uncomment to see the huge nb of parameters ...\n",
        "#for name, param in classifier.named_parameters():\n",
        "   #print(\"PARAM named %s, of shape %s\" % (name, str(param.shape)))\n",
        "   #print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LeXSxEXexy5"
      },
      "source": [
        "#### Test of forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useless to compute gradients when testing\n",
        "with torch.no_grad():\n",
        "    # toggle train mode off\n",
        "    classifier.eval()\n",
        "    for b_tid_seqs, b_tg_trks,b_lemmas,b_labels in wsd_data['train'].make_batches(32, shuffle_data=True):\n",
        "        b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier.device)\n",
        "        b_tg_trks = torch.tensor(b_tg_trks, device=classifier.device)\n",
        "        #log_probs = classifier(b_tid_seqs, b_tg_trks)\n",
        "        mask = b_lemmas\n",
        "        log_probs = classifier(b_tid_seqs,b_tg_trks)\n",
        "        gold = b_labels[0] #.item()\n",
        "        gold_lemma = b_lemmas[0]\n",
        "        print(f'first ex : {gold_lemma}')\n",
        "        print(\"GOLD LABEL of first ex %d ( = %s)\" % (gold, i2label[gold]))\n",
        "        print(\"LOG_PROBS before training: %s\\n\\n\" % str(log_probs[0]))\n",
        "        break\n"
      ],
      "metadata": {
        "id": "rKgXF9f0RZGp",
        "outputId": "cdf0c871-ae21-47db-a0eb-912317115c65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first ex : s'allier\n",
            "GOLD LABEL of first ex 8 ( = Make_agreement_on_action)\n",
            "LOG_PROBS before training: tensor([-6.6512, -7.3057, -5.7636, -4.9219, -6.8631, -7.7480, -5.4645, -6.6998,\n",
            "        -3.6390, -6.2735, -3.7086, -7.2166, -6.8038, -5.0077, -4.6372, -6.7378,\n",
            "        -6.0674, -6.4070, -4.7714, -5.9692, -5.2273, -6.4065, -4.0394, -2.2338,\n",
            "        -4.8827, -5.9193, -5.4862, -4.4570, -5.4531, -5.9702, -4.5276, -4.4424,\n",
            "        -4.9234, -5.7870, -5.0041, -6.1230, -4.4383, -4.7733, -5.6809, -4.4642,\n",
            "        -2.4661, -6.7032, -5.8991, -6.1683, -6.8110, -3.3326, -3.8445, -7.9213,\n",
            "        -4.0945, -4.6576, -7.1300, -5.5480, -4.3570, -4.3896, -4.5131, -4.0048,\n",
            "        -6.3958, -4.8002, -5.4079, -7.4110, -6.5508, -4.2832, -3.7300, -5.5400,\n",
            "        -6.1726, -4.8362, -4.4931, -6.2256, -5.8992, -7.2413, -5.7279, -4.9946,\n",
            "        -5.8178, -5.2648, -5.9453, -7.0204, -4.9609, -4.3784, -6.1446, -4.5937,\n",
            "        -4.4972, -5.9254, -6.3766, -4.2953, -5.9825, -5.0797, -4.8794, -6.1832,\n",
            "        -4.5445, -5.6917, -6.2342, -3.2223, -5.3736, -5.9121, -6.9648, -2.9838,\n",
            "        -4.6563, -5.3115, -5.5499, -5.9452, -5.0163, -7.4239, -5.3953, -6.8540,\n",
            "        -4.8919, -2.6048, -6.5160], device='cuda:0')\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aikfNi0zexy9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOKrCRPFexy-"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "\n",
        "import time\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.0005\n",
        "nb_epoch = 30\n",
        "epoch_id = 0\n",
        "\n",
        "time_0 = time.time()\n",
        "num_labels = len(i2label)\n",
        "DEVICE = 'cuda'\n",
        "classifier = WSDClassifier(num_labels, DEVICE, flaubert_model, flaubert_config,MLP = False)\n",
        "# ATTENTION ,USE MLP WILL SOME TIME RAISE OUT OF CUDA MEMORY ERROR\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "# SGD is quicker (more convenient for debug phase)\n",
        "#optimizer = optim.SGD(classifier.parameters(), lr=LR)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=LR)\n",
        "\n",
        "config_name = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
        "out_model_file = './' + config_name + '.model'\n",
        "out_log_file = './' + config_name + '.log'\n",
        "\n",
        "\n",
        "# losses at each epoch (on train / on validation set)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "min_val_loss = None\n",
        "min_val_accuracy = 0\n",
        "# to speed up during debug: train on dev\n",
        "#train_data = wsd_data['dev'] # data['train']\n",
        "train_data = wsd_data['train']\n",
        "val_data = wsd_data['dev']\n",
        "val_batch_size = len(wsd_data['dev'].tid_seqs)\n",
        "lemmas = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "while(epoch_id < nb_epoch):\n",
        "  classifier.train()\n",
        "  time_0 = time.time()\n",
        "  \n",
        "  for b_tid_seqs, b_tg_trks,b_lemmas,b_labels in train_data.make_batches(BATCH_SIZE, shuffle_data=True):\n",
        "        b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier.device)\n",
        "        b_tg_trks = torch.tensor(b_tg_trks, device=classifier.device)\n",
        "        gold = torch.tensor(b_labels, device=classifier.device)\n",
        "        lemmas = b_lemmas\n",
        "        optimizer.zero_grad()\n",
        "        log_probs = classifier(b_tid_seqs,b_tg_trks)\n",
        "        loss = loss_function(log_probs,gold)\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  val_accuracy,avg_val_loss = classifier.evaluate(val_data,return_loss = True,batch_size = val_batch_size)\n",
        "  print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "  print(f\"Average loss on training set: {sum(train_losses) / len(train_losses)}.\")\n",
        "  print(f\"Average loss on dev set: {avg_val_loss}.\")\n",
        "  print(f\"Average accuracy on dev set: {val_accuracy}.\")\n",
        "  if val_accuracy <= min_val_accuracy:  #early stopping and roll back\n",
        "    print('-- Aaccuracy down,roll back and stop --')\n",
        "    classifier.load_state_dict(torch.load('classifier_params.pt'))\n",
        "    break\n",
        "  else:\n",
        "    min_val_accuracy = val_accuracy\n",
        "    torch.save(classifier.state_dict(), 'classifier_params.pt')\n",
        "    \n",
        "\n",
        "  duration = time.time() - time_0\n",
        "  print(f\"{duration} s elapsed (i.e. {duration / (epoch_id + 1)} s/epoch)\")\n",
        "  epoch_loss = []\n",
        "  epoch_id += 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHt6rTougG0K"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHYxcJBNexzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131a50bc-0ccb-4553-bf20-071b96a7416f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0029795158188790083, 0.18642462971275686)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "test_data = wsd_data['dev']\n",
        "batch_size = len(test_data.tid_seqs)\n",
        "classifier.evaluate(test_data,batch_size)\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Transformers-wsd.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
